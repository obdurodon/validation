TODO
----- version 1 -------
- trap connection time out errors and report server busy, try again soon
- error files before clean files
    report html files as processed if invalid or broken links
    then list summary of files with valid html and links (just filenames)
    do same for css
- for link checking, check for 404s for absolute file paths and check manually
- check portability
- check file naming and extensions (report html ext as bad)
    no spaces, no punctuation other than -,_,.
    must be .xhtml (legacy exception)
    for .html report filename error and don't validate
- exclude creative commons license in link checking
- rename excluded domains to scheme name and include colon
- rename man_domains to include host
- clean up and add comments
- add to documentation about how includes are processed in validation (line number issues)
----- version 2 -------
- php support....eventually (option to ignore php) ----- 
    check mime type in headers (needs to be application/xhtml+xml, legacy is text/html)
    use mime type for normal xhtml/html as well

OPTIONS (all completed)
- make checking links an option to save time checking static pages? (put on hold while dealing with robots issue)
- save output to file (-s)
- switch for legacy code
- use specified directory
- project vs public directory
- debug mode with all output
- turn off html validation
- turn off css validation

HELPFUL COMMANDS

directory for executable:
/usr/local/bin
